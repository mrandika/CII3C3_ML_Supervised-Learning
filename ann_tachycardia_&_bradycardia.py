# -*- coding: utf-8 -*-
"""ANN - Tachycardia & Bradycardia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cdj3X9BcMwuobh2Qf9md13X5XhOSpQKA
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from collections import Counter

from imblearn.over_sampling import SMOTE

from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras.utils.np_utils import to_categorical
from keras.utils.vis_utils import plot_model

df = pd.read_csv("https://raw.githubusercontent.com/mrandika/CII3C3_ML_Supervised-Learning/main/arrhythmia_data.csv")

df.info()

df.describe()

"""# Pre Processing

"""

# Delete unrelated data
columnIndex = df[(df["class"].isin([2,3,4,7,8,9,10,11,12,13,14,15,16]))].index
df.drop(columnIndex, inplace=True)

# Only Register Normal, Tachycardia, and Bardycardia
df.loc[df["class"] == 1, "class"] = 0
df.loc[df["class"] == 5, "class"] = 1
df.loc[df["class"] == 6, "class"] = 2

# Remove any duplicate data
df = df.drop_duplicates()

# Fix missing values with NaN
df.replace('?', np.NaN, inplace=True)

# Replace missing value with median data
imp_mean = SimpleImputer(missing_values=np.NaN, strategy='median')
imputer = imp_mean.fit(df)

df_imp = imputer.transform(df)
df = pd.DataFrame(df_imp, columns=df.columns)

# Set column usage
df_column_correlated_definition = [
    'sex', 'qrs', 'p-r_interval', 'q-t_interval', 't_interval', 'p_interval', 'bpm', 
    'd1_ampl_p_wave', 'd1_ragged_p', 'd1_qrsa',
    'd2_ampl_p_wave', 'd2_qrsa',
    'd3_ampl_p_wave', 'd3_qrsa',
    'avr_ampl_p_wave', 'avr_qrsa',
    'avr_ragged_t', 
    'avl_ampl_p_wave', 'avl_qrsa',
    'avf_ampl_p_wave', 'avf_diphasic_derivation_p_wave', 'avf_qrsa',
    'v6_ragged_p',
    'class'
]

df = df[df_column_correlated_definition]

corrmat = df.corr()
plt.figure(figsize=(20,20))
g = sns.heatmap(df[corrmat.index].corr(), annot=True, cmap="RdYlGn")

# Get sample from each class
normal = df.loc[df['class'] == 0].sample(n=15)
tachycardia = df.loc[df['class'] == 1].sample(n=2)
bradycardia = df.loc[df['class'] == 2].sample(n=3)

# Combine to 1 dataframe
df_test = pd.concat([normal, tachycardia, bradycardia])

# Drop the data from test set from main set
df = pd.concat([df, df_test]).drop_duplicates(keep=False)

# Separate attribute and label
df_data = df.drop("class", axis="columns")
df_class = df["class"]

# Separate test attribute and label
df_test_data = df_test.drop("class", axis="columns")
df_test_class = df_test["class"]

y = LabelEncoder().fit_transform(df_class)

# summarize distribution
counter = Counter(y)

# plot the distribution
plt.title("Class distribution")
plt.bar(counter.keys(), counter.values())
plt.show()

# SMOTE oversample
# Only oversample the tachycardia and bradycardia
# Use K-Neighbors of 4
strategy = {1:260, 2:230}
oversample = SMOTE(sampling_strategy=strategy, k_neighbors=4)
df_data, df_class = oversample.fit_resample(df_data, df_class)

y = LabelEncoder().fit_transform(df_class)

# summarize distribution
counter = Counter(y)

# plot the distribution
plt.title("Class distribution")
plt.bar(counter.keys(), counter.values())
plt.show()

"""# Data Splitting"""

# Change existing classification to categorical
df_class_cat = to_categorical(df_class)

# Split the training and test set
X_train, X_validation, y_train, y_validation = train_test_split(df_data, df_class_cat, test_size = 0.2, shuffle = True)

# Scale the training, validation, and testing data
sc = StandardScaler()
X_train = pd.DataFrame(sc.fit_transform(X_train))
X_validation = pd.DataFrame(sc.transform(X_validation))
df_test_data = pd.DataFrame(sc.transform(df_test_data))

print(f"Train data: {len(X_train)}")
print(f"Validation data: {len(X_validation)}")

# Setup model
model = Sequential()

model.add(Dense(23, input_shape = (23,), activation = "relu"))
model.add(Dense(10, activation = "relu"))
model.add(Dropout(0.1))
model.add(Dense(3, activation = "softmax"))
model.compile(Adam(learning_rate = 0.003), "categorical_crossentropy", metrics = ["accuracy"])
model.summary()

train = model.fit(X_train, y_train, verbose=1, epochs=100, batch_size=100, validation_data=(X_validation, y_validation))

plot_model(model, show_shapes=True, show_layer_names=True)

plt.plot(train.history["loss"], label="Training Loss")
plt.plot(train.history["val_loss"], label="Validation Loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(loc="best")
plt.show

predict = model.predict(df_test_data)

class_name = ['normal', 'sinus-tachycardia', 'brady-cardia', 'other']
predicted_class = [np.argmax(i) for i in predict]

for i in range(len(predict)):
  class_calculation = predict[i]
  class_prediction = class_name[predicted_class[i]]

  class_list = df_test_class.tolist()
  actual_class = class_name[int(class_list[i])]

  print(f"Data-{i} => {class_prediction}, actual => {actual_class}")

print(f"Accuracy: {accuracy_score(df_test_class, predicted_class)}%")

confusion_matrix(df_test_class, predicted_class)

print(classification_report(df_test_class, predicted_class))